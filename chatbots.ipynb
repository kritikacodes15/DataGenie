{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58d2dffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() # a loading all the environment variable\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "410689b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Data_Learnings\\Langchain\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model = ChatGroq(model='openai/gpt-oss-20b',groq_api_key=groq_api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00989873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hi Kritika! üëã  \\nIt‚Äôs great to meet an engineer who loves building and designing new things. What kind of projects are you working on right now? Anything exciting‚Äîwhether it‚Äôs a prototype, a design challenge, or a software/firmware project? I‚Äôd love to hear more and see if there‚Äôs any way I can help, whether it‚Äôs brainstorming ideas, troubleshooting, or just chatting about the latest tech trends.', additional_kwargs={'reasoning_content': 'We need to respond. The user introduced themselves. We should respond warmly, ask about projects, maybe offer assistance.'}, response_metadata={'token_usage': {'completion_tokens': 121, 'prompt_tokens': 90, 'total_tokens': 211, 'completion_time': 0.121084535, 'completion_tokens_details': {'reasoning_tokens': 24}, 'prompt_time': 0.007461783, 'prompt_tokens_details': None, 'queue_time': 0.052899777, 'total_time': 0.128546318}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_e99e93f2ac', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--ed30bbf9-d762-4479-a4db-d8e025122ee7-0', usage_metadata={'input_tokens': 90, 'output_tokens': 121, 'total_tokens': 211, 'output_token_details': {'reasoning': 24}})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "model.invoke([HumanMessage(content='Hii , My name is Kritika i am an engineer i like building and designing new things')]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b181f37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='You‚Äôre **Kritika** ‚Äì an engineer and physicist who‚Äôs worn many hats across a wide spectrum of cutting‚Äëedge fields. In your career you‚Äôve dived into:\\n\\n- **Astroparticle & High‚ÄëEnergy Physics** ‚Äì probing the universe at the smallest scales and highest energies  \\n- **Business Analytics & Data Science** ‚Äì turning raw data into actionable insights for organizations  \\n- **Generative AI (GenAI)** ‚Äì building and refining models that create new content, solutions, and ideas  \\n- **Material Science & Microelectronics** ‚Äì designing and optimizing the very building blocks of modern technology  \\n\\nSo, in short: a multidisciplinary engineer‚Äëphysicist who blends fundamental science with applied tech to innovate across both academia and industry.', additional_kwargs={'reasoning_content': 'User: \"Hey!! What is my name and what do I do?\" They want answer: name Kritika, and what they do: engineer and physicist working in diverse fields: astroparticle, high energy physics, business analytics, GenAI, data science, material science, electronics, microelectronics. So answer accordingly.'}, response_metadata={'token_usage': {'completion_tokens': 225, 'prompt_tokens': 183, 'total_tokens': 408, 'completion_time': 0.238281757, 'completion_tokens_details': {'reasoning_tokens': 67}, 'prompt_time': 0.015490207, 'prompt_tokens_details': None, 'queue_time': 0.049072932, 'total_time': 0.253771964}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_e99e93f2ac', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--114e23b3-d362-4b65-8db9-b2ec65a87d08-0', usage_metadata={'input_tokens': 183, 'output_tokens': 225, 'total_tokens': 408, 'output_token_details': {'reasoning': 67}})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "### We have list of messages \n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hii, My name is Kritika. I am an engineer and a physicist. I worked in diverse fields like astroparticle, high energy physics, business analytics, GenAI, data science, material science, electronics, microelectronics.\"),\n",
    "        AIMessage(content='Hey Kritika! üëã It‚Äôs great to meet an engineer who‚Äôs passionate about building and designing new things. What kind of projects are you working on right now? Anything exciting you‚Äôre dreaming up?'),\n",
    "        HumanMessage(content=\"Hey!! What is my name and what do I do?\")\n",
    "    ]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18183a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory    \n",
    "store = {}\n",
    "## function will be able to retrieve the chat message history that is already present\n",
    "def get_session_history(session_id:str)->BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]    \n",
    "## we are using this RunnableWithMessageHistory so that we interact with model based on our chat history\n",
    "# IMPORTANT: specify message keys\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f01a41ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(model,get_session_history,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99335462",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"chat1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92e6538c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Data_Learnings\\Langchain\\venv\\lib\\site-packages\\pydantic\\v1\\main.py:1054: UserWarning: LangSmith now uses UUID v7 for run and trace identifiers. This warning appears when passing custom IDs. Please use: from langsmith import uuid7\n",
      "            id = uuid7()\n",
      "Future versions will require UUID v7.\n",
      "  input_data = validator(cls_, input_data)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    {\"messages\": [\n",
    "        HumanMessage(content=\"Hii my name is Kritika!! , I am Data Engineer. I love statistical storytelling.\")\n",
    "    ]},\n",
    "    config=config\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f459613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='- **Your name:** Kritika  \\n- **What you do:** You‚Äôre a Data Engineer who enjoys turning data into compelling statistical stories.', additional_kwargs={'reasoning_content': 'User asked: \"What is my name? and What do i do ?\" They previously introduced themselves: \"Hii my name is Kritika!! , I am Data Engineer. I love statistical storytelling.\" So answer: name Kritika, occupation Data Engineer, loves statistical storytelling. Provide concise.'}, response_metadata={'token_usage': {'completion_tokens': 96, 'prompt_tokens': 208, 'total_tokens': 304, 'completion_time': 0.098274603, 'completion_tokens_details': {'reasoning_tokens': 59}, 'prompt_time': 0.017581682, 'prompt_tokens_details': None, 'queue_time': 0.047780718, 'total_time': 0.115856285}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_e99e93f2ac', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--96769bca-addb-4f35-9916-06853f4b00bc-0', usage_metadata={'input_tokens': 208, 'output_tokens': 96, 'total_tokens': 304, 'output_token_details': {'reasoning': 59}})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"What is my name? and What do i do ?\")]\n",
    "    },\n",
    "    config=config\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "410ff814",
   "metadata": {},
   "outputs": [],
   "source": [
    "config1 = {\"configurable\": {\"session_id\": \"chat2\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b50682a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I‚Äôm not sure what your name is‚Äîcould you let me know? And yes, Dr. Strange is the Marvel superhero who uses mystic arts to protect the world. If you have any questions about him or anything else, feel free to ask!', additional_kwargs={'reasoning_content': 'The user asks: \"What is my name? Do you remember Dr. Strange?\" We need to respond. We don\\'t know the user\\'s name; we cannot assume. We should ask for their name or say we don\\'t know. Also mention Dr. Strange. The user might be referencing the Marvel character. We should respond politely. Also abide by policy: we can ask for clarification. There\\'s no disallowed content. Just respond.'}, response_metadata={'token_usage': {'completion_tokens': 146, 'prompt_tokens': 84, 'total_tokens': 230, 'completion_time': 0.144462024, 'completion_tokens_details': {'reasoning_tokens': 86}, 'prompt_time': 0.00398121, 'prompt_tokens_details': None, 'queue_time': 0.155999119, 'total_time': 0.148443234}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_e23fc997ca', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--bda07aae-1a27-4b26-a00f-900b1cc39851-0', usage_metadata={'input_tokens': 84, 'output_tokens': 146, 'total_tokens': 230, 'output_token_details': {'reasoning': 86}})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"What is my name? Do you remember Dr. Strange ? \")]\n",
    "    },\n",
    "    config=config1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "259dc6ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='You‚Äôre absolutely right‚ÄîDr.\\u202fStrange is one of Marvel‚Äôs most iconic heroes, wielding mystic arts to protect Earth (and occasionally the multiverse). And it‚Äôs great to hear that Onkan\\u202fRoy is a fan of Marvel characters! Which ones are his favorites? Or is there a particular storyline or movie he‚Äôs been excited about lately? Let me know‚Äîhappy to dive into the Marvel universe with you!', additional_kwargs={'reasoning_content': 'The user says: \"Hey GPT , Dr Strange is Marvel hero. Onkan Roy loves Marvel characters.\" They likely want me to respond acknowledging that Dr Strange is Marvel hero and that Onkan Roy loves Marvel characters. Maybe they want me to remember Onkan Roy\\'s interest. So I should respond acknowledging and maybe ask about Onkan Roy or their favorite Marvel characters. Or ask for more context. The user hasn\\'t asked a question but might be setting context. I should respond politely, maybe ask what they\\'d like to talk about. Also note that I should not reveal that I cannot remember previous conversations, but I can mention that I don\\'t have memory across sessions. But I can say \"I don\\'t have memory of past conversation, but I can remember within this session.\" So respond accordingly.'}, response_metadata={'token_usage': {'completion_tokens': 253, 'prompt_tokens': 164, 'total_tokens': 417, 'completion_time': 0.255895489, 'completion_tokens_details': {'reasoning_tokens': 157}, 'prompt_time': 0.018562796, 'prompt_tokens_details': None, 'queue_time': 0.049066034, 'total_time': 0.274458285}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_e99e93f2ac', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--918b309f-fd8a-4985-a9c1-3c33bbe715d5-0', usage_metadata={'input_tokens': 164, 'output_tokens': 253, 'total_tokens': 417, 'output_token_details': {'reasoning': 157}})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"Hey GPT , Dr Strange is Marvel hero. Onkan Roy loves Marvel caharcters.\")]\n",
    "    },\n",
    "    config=config1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8119fd05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I don‚Äôt have that detail in my memory‚Äîcould you tell me what it is that Onkan\\u202fRoy loves most about Marvel? (For example, whether it‚Äôs the characters, the story arcs, the art style, the films, the comics, etc.) That way I can chat more specifically about it!', additional_kwargs={'reasoning_content': 'The user says: \"Hey GPT !! Do you remember what Onkan Roy loves about Marvel ?\" The user earlier said \"Onkan Roy loves Marvel caharcters.\" They didn\\'t specify what he loves about Marvel. They might be asking if I recall earlier conversation. But earlier we didn\\'t get details. We need to respond politely, asking for clarification. Also we should not fabricate. So respond: I don\\'t have that info, can you tell me?'}, response_metadata={'token_usage': {'completion_tokens': 163, 'prompt_tokens': 275, 'total_tokens': 438, 'completion_time': 0.162582092, 'completion_tokens_details': {'reasoning_tokens': 91}, 'prompt_time': 0.033462502, 'prompt_tokens_details': None, 'queue_time': 0.077427238, 'total_time': 0.196044594}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_e99e93f2ac', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--73b54181-1d71-49f5-9ea8-2ef3fc2e17ac-0', usage_metadata={'input_tokens': 275, 'output_tokens': 163, 'total_tokens': 438, 'output_token_details': {'reasoning': 91}})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"Hey GPT !! Do you remember what Onkan Roy loves about Marvel ?\")]\n",
    "    },\n",
    "    config=config1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "440bb027",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prompt Templates\n",
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant. Answer all the questions to the best of your ability.\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ]\n",
    ")\n",
    "chain = prompt|model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ebe084a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello! üëã I'm happy to dive into high‚Äëenergy physics with you. Whether you're curious about the Standard Model, particle accelerators, recent discoveries at the LHC, cosmology connections, or any specific theory or experiment, just let me know what you'd like to discuss or ask!\", additional_kwargs={'reasoning_content': 'We need to respond as a helpful assistant, about high energy physics. Probably a conversation. The user says \"Hello Let us interact on High Energy Physics\". We can respond with greeting and ask what topic or question. Let\\'s produce a friendly response.'}, response_metadata={'token_usage': {'completion_tokens': 117, 'prompt_tokens': 99, 'total_tokens': 216, 'completion_time': 0.115997952, 'completion_tokens_details': {'reasoning_tokens': 50}, 'prompt_time': 0.008220101, 'prompt_tokens_details': None, 'queue_time': 0.165314648, 'total_time': 0.124218053}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_e99e93f2ac', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--bdbda5a8-ba07-4682-8f93-56341e3c76c6-0', usage_metadata={'input_tokens': 99, 'output_tokens': 117, 'total_tokens': 216, 'output_token_details': {'reasoning': 50}})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'messages':[HumanMessage(content='Hello Let us interact on High Energy Physics')]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "419aef77",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history=RunnableWithMessageHistory(chain,get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0425d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "config9 = {'configurable':{'session_id':'chat3'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "accef82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content='Hello , Let us talk about Collidor Physics ')],\n",
    "     config=config9\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f9a59d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sure thing!\\u202fLet‚Äôs dive into **collider physics**‚Äîthe science that explores what happens when particles are smashed together at near‚Äëlight speeds in large accelerators.\\n\\n---\\n\\n## Quick Overview\\n\\n| Topic | What it is | Why it matters |\\n|-------|------------|----------------|\\n| **Particle colliders** | Machines (e.g., LHC, RHIC, Fermilab‚Äôs Tevatron) that accelerate beams of particles (protons, electrons, ions) to high energies and collide them. | They let us probe the fundamental constituents of matter and the forces that bind them. |\\n| **Detectors** | Complex arrays of sensors (tracking chambers, calorimeters, muon systems, etc.) that record the debris from collisions. | They reconstruct the events, allowing us to infer what happened inside the collision point. |\\n| **Standard Model tests** | Precise measurements of known particles (W/Z bosons, top quark, Higgs boson) and interactions. | Validates or challenges the Standard Model, the current best theory of particle physics. |\\n| **Beyond the Standard Model (BSM)** | Searches for new particles or phenomena (supersymmetry, dark matter candidates, extra dimensions). | Aims to answer open questions like the nature of dark matter, matter‚Äìantimatter asymmetry, etc. |\\n| **High‚Äëenergy physics & cosmology** | Connections between collider results and cosmological observations (early universe conditions, baryogenesis). | Helps bridge the gap between particle physics and cosmology. |\\n\\n---\\n\\n## Where to Start?\\n\\n1. **Basic Concepts**  \\n   * What are the key particles and forces?  \\n   * How does kinetic energy translate to mass in collisions (E=mc¬≤)?  \\n\\n2. **Collider Types**  \\n   * **Hadron colliders** (protons, ions): LHC, RHIC.  \\n   * **Lepton colliders** (electrons, positrons): LEP, proposed ILC, CLIC.  \\n\\n3. **Detectors & Data**  \\n   * How do detectors reconstruct tracks, energy deposits, and identify particles?  \\n   * Data‚Äëanalysis pipelines: trigger systems, reconstruction software, statistical methods.  \\n\\n4. **Key Discoveries**  \\n   * Higgs boson (2012).  \\n   * Precise measurements of the top quark mass.  \\n   * Searches for supersymmetric particles, dark matter, etc.  \\n\\n5. **Future Directions**  \\n   * High‚Äëluminosity upgrades (HL‚ÄëLHC).  \\n   * Next‚Äëgeneration colliders (FCC, CEPC, SppC).  \\n   * Novel technologies (plasma wake‚Äëfield acceleration, muon colliders).  \\n\\n---\\n\\n## What would you like to explore?\\n\\n- **Historical milestones** (e.g., discovery of the W/Z, Higgs).  \\n- **Technical aspects** (accelerator physics, detector design).  \\n- **Theoretical implications** (BSM theories, dark matter).  \\n- **Recent experimental results** (latest LHC data releases).  \\n- **Future collider projects** and their science goals.  \\n\\nLet me know which angle interests you, or feel free to ask a specific question‚Äîwhether it‚Äôs about the physics behind the collisions, the engineering marvels that make them possible, or the exciting new frontiers on the horizon!'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82055645",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I don‚Äôt have the ability to recall past conversations, so I‚Äôm not sure what we were discussing last time. But I‚Äôm happy to pick up where you‚Äôd like! Just let me know the topic or any specific questions you have, and we can dive right back in.', additional_kwargs={'reasoning_content': 'User asks: \"Hello , Do you remember the topic we were interacting in last session\". They likely want to resume previous conversation. But we have no memory of prior sessions. We must politely respond that we don\\'t have memory. Offer to continue.'}, response_metadata={'token_usage': {'completion_tokens': 115, 'prompt_tokens': 809, 'total_tokens': 924, 'completion_time': 0.113977397, 'completion_tokens_details': {'reasoning_tokens': 50}, 'prompt_time': 0.062741576, 'prompt_tokens_details': None, 'queue_time': 0.053161644, 'total_time': 0.176718973}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_e99e93f2ac', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--b8791407-b85e-4fee-a138-3992b76464c5-0', usage_metadata={'input_tokens': 809, 'output_tokens': 115, 'total_tokens': 924, 'output_token_details': {'reasoning': 50}})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    [HumanMessage(content='Hello , Do you remember the topic we were interacting inlast session')],\n",
    "     config=config9\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "45ed2956",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant. Answer all the questions to the best of your ability. in {language}\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ]\n",
    ")\n",
    "chain = prompt|model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b3b2cd18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='‡§®‡§Æ‡§∏‡•ç‡§§‡•á Kritika ‡§ú‡•Ä! ‡§Ü‡§™‡§∏‡•á ‡§Æ‡§ø‡§≤‡§ï‡§∞ ‡§ñ‡•Å‡§∂‡•Ä ‡§π‡•Å‡§à‡•§ ‡§Ü‡§™ ‡§è‡§ï ‡§á‡§Ç‡§ú‡•Ä‡§®‡§ø‡§Ø‡§∞ ‡§π‡•à‡§Ç‚Äî‡§¨‡§π‡•Å‡§§ ‡§¨‡§¢‡§º‡§ø‡§Ø‡§æ! ‡§Æ‡•à‡§Ç ‡§Ü‡§™‡§ï‡•Ä ‡§ï‡§ø‡§∏ ‡§™‡•ç‡§∞‡§ï‡§æ‡§∞ ‡§∏‡§π‡§æ‡§Ø‡§§‡§æ ‡§ï‡§∞ ‡§∏‡§ï‡§§‡§æ/‡§∏‡§ï‡§§‡•Ä ‡§π‡•Ç‡§Å? ‡§ö‡§æ‡§π‡•á ‡§µ‡§π ‡§ï‡§ø‡§∏‡•Ä ‡§§‡§ï‡§®‡•Ä‡§ï‡•Ä ‡§µ‡§ø‡§∑‡§Ø ‡§™‡§∞ ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§π‡•ã, ‡§™‡§∞‡§ø‡§Ø‡•ã‡§ú‡§®‡§æ ‡§∏‡•á ‡§∏‡§Ç‡§¨‡§Ç‡§ß‡§ø‡§§ ‡§∏‡§≤‡§æ‡§π ‡§π‡•ã, ‡§Ø‡§æ ‡§ï‡•Å‡§õ ‡§î‡§∞, ‡§¨‡§∏ ‡§¨‡§§‡§æ‡§á‡§è‡•§', additional_kwargs={'reasoning_content': 'We need to respond in Hindi. The user says: \"Hello my name is Kritika , I am an engineer\". We need to respond appropriately, maybe greet and ask how we can help. The instruction: \"Answer all the questions to the best of your ability. in Hindi\". There\\'s no question. So we can respond politely, maybe ask what assistance they need. Use Hindi.'}, response_metadata={'token_usage': {'completion_tokens': 153, 'prompt_tokens': 104, 'total_tokens': 257, 'completion_time': 0.154983391, 'completion_tokens_details': {'reasoning_tokens': 78}, 'prompt_time': 0.005687704, 'prompt_tokens_details': None, 'queue_time': 0.048993126, 'total_time': 0.160671095}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_e99e93f2ac', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--be98c38e-3d38-48d7-a3ec-a96cf6bab9aa-0', usage_metadata={'input_tokens': 104, 'output_tokens': 153, 'total_tokens': 257, 'output_token_details': {'reasoning': 78}})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'messages':[HumanMessage(content='Hello my name is Kritika , I am an engineer')],'language':'Hindi'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af597658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Absolutely! Let‚Äôs walk through the three concepts‚Äî**LLM workflow**, **Retrieval‚ÄëAugmented Generation (RAG)**, and **AI Agents**‚Äîand see how they fit together in a modern AI stack. I‚Äôll cover the basics, the typical pipeline, key components, best‚Äëpractice patterns, and some practical examples. Feel free to ask follow‚Äëup questions on any part.\\n\\n---\\n\\n## 1. LLM Workflow\\n\\nAn **LLM workflow** is the end‚Äëto‚Äëend pipeline that takes user input (or another source of data) and produces a final response or action using a large language model. A typical workflow looks like this:\\n\\n```\\nUser Input  ‚Üí  Pre‚Äëprocessing  ‚Üí  Prompt Engineering  ‚Üí  LLM Inference  ‚Üí  Post‚Äëprocessing  ‚Üí  Output\\n```\\n\\n### 1.1. Pre‚Äëprocessing\\n\\n| Step | What it does | Why it matters |\\n|------|--------------|----------------|\\n| **Tokenization** | Breaks text into tokens the model understands. | Enables efficient computation. |\\n| **Cleaning** | Removes noise (e.g., extra whitespace, malformed HTML). | Improves model confidence. |\\n| **Contextualization** | Adds system messages or prior conversation history. | Provides context for better responses. |\\n| **Formatting** | Converts data into a format the model expects (JSON, markdown, etc.). | Avoids hallucinations or misinterpretation. |\\n\\n### 1.2. Prompt Engineering\\n\\n| Technique | Purpose | Example |\\n|-----------|---------|---------|\\n| **Prompt Templates** | Standardize the structure of prompts. | `\"Question: {question}\\\\nAnswer:\"` |\\n| **Few‚Äëshot Prompting** | Show examples to steer the model. | `\"Example 1: ...\\\\nExample 2: ...\\\\nNow: {question}\"` |\\n| **Chain‚Äëof‚ÄëThought (CoT)** | Encourage the model to reason step‚Äëby‚Äëstep. | `\"First, calculate X. Then, Y.\"` |\\n| **System / Role Messages** | Set the model‚Äôs persona or constraints. | `\"You are a helpful assistant that only provides factual answers.\"` |\\n\\n### 1.3. LLM Inference\\n\\n| Parameter | Typical Choices | Impact |\\n|-----------|-----------------|--------|\\n| **Temperature** | 0.0‚Äì1.0 | Controls randomness (lower = deterministic). |\\n| **Top‚Äëp (nucleus sampling)** | 0.8‚Äì1.0 | Filters unlikely tokens. |\\n| **Max Tokens** | 256‚Äì2048 | Limits output length. |\\n| **Stop Sequences** | `[\"\\\\n\"]`, `[\"###\"]` | Signals the model when to stop. |\\n\\n### 1.4. Post‚Äëprocessing\\n\\n| Task | Why |\\n|------|-----|\\n| **Detokenization** | Convert tokens back to human‚Äëreadable text. |\\n| **Sanitization** | Strip disallowed content, profanity, or hallucinated facts. |\\n| **Formatting** | Convert to Markdown, JSON, or other target format. |\\n| **Validation** | Cross‚Äëcheck facts against a knowledge base or rule set. |\\n\\n### 1.5. Output\\n\\nThe final response can be:\\n\\n- Text (chatbot reply, article, code snippet)\\n- Structured data (JSON, CSV)\\n- An action (API call, database write)\\n- A chain of actions (for multi‚Äëstep reasoning)\\n\\n---\\n\\n## 2. Retrieval‚ÄëAugmented Generation (RAG)\\n\\nRAG augments a generative LLM with external, up‚Äëto‚Äëdate knowledge. The idea: retrieve relevant documents (or embeddings) first, then feed them into the LLM as context.\\n\\n### 2.1. High‚ÄëLevel Architecture\\n\\n```\\nInput  ‚Üí  Retrieval Module  ‚Üí  Retrieved Context  ‚Üí  Prompt (with context)  ‚Üí  LLM  ‚Üí  Post‚Äëprocessing  ‚Üí  Output\\n```\\n\\n### 2.2. Retrieval Module\\n\\n| Option | Description | Pros / Cons |\\n|--------|-------------|-------------|\\n| **Vector Search** (FAISS, Pinecone) | Embeds documents ‚Üí nearest‚Äëneighbour search. | Fast, handles large corpora. Requires embedding model. |\\n| **Keyword Search** (Elasticsearch) | Traditional inverted index. | Simple, good for exact matches. |\\n| **Hybrid** | Combines vector + keyword. | Often best of both worlds. |\\n| **Knowledge Graph Query** | Graph‚Äëbased retrieval. | Rich semantic relations but more complex. |\\n\\n### 2.3. Prompt Design in RAG\\n\\n- **Include the most relevant snippets** (e.g., top‚Äëk 3‚Äì5 passages).\\n- **Provide source IDs** so you can trace back to the original document.\\n- **Use a ‚Äúcontext‚Äù block**:  \\n  ```\\n  Context:\\n  1. Passage from doc A ‚Ä¶\\n  2. Passage from doc B ‚Ä¶\\n  \\n  Question: ...\\n  ```\\n- **Ask the model to cite**: `\"Please cite the source of each fact.\"`\\n\\n### 2.4. Example Flow\\n\\n1. **User asks:** ‚ÄúWhat is the current unemployment rate in Germany?‚Äù  \\n2. **Retrieval:** Search a recent economic database ‚Üí fetch latest report.  \\n3. **Prompt:**  \\n   ```\\n   Context:\\n   1. \"Germany\\'s unemployment rate in Q1 2025 is 3.2% according to the Federal Statistical Office.\"\\n   \\n   Question: What is Germany\\'s unemployment rate in 2025?\\n   ```\\n4. **LLM output:** ‚ÄúGermany‚Äôs unemployment rate in 2025 is 3.2%.‚Äù  \\n5. **Post‚Äëprocessing:** Validate that the figure matches the source; add citation.\\n\\n### 2.5. Evaluation & Challenges\\n\\n| Issue | Mitigation |\\n|-------|------------|\\n| **Hallucination** | Rely on retrieved facts; prompt for citations. |\\n| **Latency** | Cache embeddings; batch queries; use efficient index. |\\n| **Cold‚Äëstart** | Pre‚Äëpopulate with high‚Äëquality seed documents. |\\n| **Coverage** | Periodically update the knowledge base. |\\n| **Privacy** | Ensure retrieved data complies with regulations. |\\n\\n---\\n\\n## 3. AI Agents\\n\\nAn **AI Agent** is an autonomous system that perceives its environment, makes decisions, and takes actions‚Äîoften using an LLM as its core reasoning engine. Agents can be simple (responding to a single prompt) or complex (multi‚Äëstep planning, interacting with APIs, learning from feedback).\\n\\n### 3.1. Core Components\\n\\n| Component | Role |\\n|-----------|------|\\n| **Perception** | Gather data (user input, sensor data, API responses). |\\n| **Planning** | Decide on a sequence of actions (via LLM or symbolic planner). |\\n| **Actuation** | Execute actions (API calls, UI updates, database writes). |\\n| **Memory** | Store past interactions, context, or world state. |\\n| **Learning** | Refine behavior over time (RLHF, supervised fine‚Äëtuning). |\\n\\n### 3.2. Agent Architectures\\n\\n| Architecture | Description | When to Use |\\n|--------------|-------------|-------------|\\n| **Single‚Äëstep** | One prompt ‚Üí one answer. | Simple Q&A or chat. |\\n| **Chain‚Äëof‚ÄëThought (CoT)** | Prompt the LLM to reason step‚Äëby‚Äëstep internally. | Complex reasoning tasks. |\\n| **Tool‚Äëusing Agent** | LLM decides which external tool/API to call. | Tasks requiring external data (e.g., booking a flight). |\\n| **Retrieval‚ÄëAugmented Agent** | Uses RAG to fetch context before acting. | Knowledge‚Äëheavy queries. |\\n| **Reinforcement‚ÄëLearning Agent** | Learns a policy to maximize reward (e.g., user satisfaction). | Long‚Äëterm interactive systems. |\\n| **Hybrid Symbolic + LLM Agent** | Combine rule‚Äëbased logic with generative reasoning. | Safety‚Äëcritical or compliance‚Äëheavy domains. |\\n\\n### 3.3. Example: A Travel Planning Agent\\n\\n1. **Perception**: User says, ‚ÄúBook a flight from NYC to Tokyo next week.‚Äù  \\n2. **Planning**: LLM decides to:\\n   - Search flight availability.\\n   - Check user preferences (budget, airlines).\\n   - Offer options.  \\n3. **Actuation**: Calls airline API, confirms booking.  \\n4. **Memory**: Stores booking ID, itinerary.  \\n5. **Learning**: Uses user feedback to improve future suggestions.\\n\\n### 3.4. Tool Integration\\n\\n| Tool | Typical Interaction |\\n|------|---------------------|\\n| **Python/CLI** | Execute code snippets. |\\n| **SQL** | Query databases. |\\n| **REST API** | GET/POST requests. |\\n| **Browser** | Web scraping or navigation. |\\n| **Custom SDKs** | E.g., Google Calendar, Stripe. |\\n\\nLLM agents often use a **‚Äútool‚Äëcalling‚Äù** pattern: the model outputs a JSON object specifying the tool name and parameters, which the orchestrator then executes. This keeps the LLM focused on decision‚Äëmaking while delegating execution to reliable systems.\\n\\n### 3.5. Safety & Reliability\\n\\n- **Guardrails**: Hard‚Äëcoded constraints (e.g., no booking for minors).  \\n- **Re‚Äëexamination**: Verify tool outputs before committing.  \\n- **Audit Trails**: Log every action for compliance.  \\n- **Fallbacks**: If the LLM fails, hand over to a human operator.  \\n\\n---\\n\\n## 4. Putting It All Together\\n\\n| Layer | Typical Tools | Example Stack |\\n|-------|---------------|---------------|\\n| **Data Layer** | Document store (Elastic, Pinecone), APIs | Wikipedia dump, internal knowledge base |\\n| **Retrieval Layer** | Vector search, keyword search | FAISS + OpenAI embeddings |\\n| **LLM Layer** | GPT‚Äë4, Claude, Llama 2 | OpenAI API, local fine‚Äëtuned model |\\n| **Agent Layer** | Prompt orchestrator, tool‚Äëcaller | LangChain, LlamaIndex, Agentic frameworks |\\n| **Application Layer** | Web UI, mobile, chatbot | Next.js, React Native, Slack bot |\\n\\n### Sample Flow\\n\\n1. **User** sends query to chat UI.  \\n2. **Agent** receives input, decides to perform a retrieval.  \\n3. **Retrieval** fetches top‚Äëk documents ‚Üí embeddings.  \\n4. **Prompt** is built: system prompt + retrieved context + user query.  \\n5. **LLM** generates answer + cites sources.  \\n6. **Agent** formats response, logs interaction, sends back to UI.\\n\\n---\\n\\n## 5. Best‚ÄëPractice Checklist\\n\\n| ‚úÖ | Item |\\n|----|------|\\n| **Define Clear Goals** | What problem does the agent solve? |\\n| **Modular Design** | Separate perception, planning, actuation. |\\n| **Version Control** | Keep prompt templates and LLM configs in Git. |\\n| **Monitoring** | Track latency, success rates, user satisfaction. |\\n| **Evaluation** | Human‚Äëin‚Äëthe‚Äëloop reviews for safety. |\\n| **Data Governance** | Respect privacy, comply with GDPR, etc. |\\n| **Continuous Retraining** | Update embeddings, fine‚Äëtune LLMs with new data. |\\n| **Fallback Paths** | Human escalation for ambiguous or critical cases. |\\n\\n---\\n\\n## 6. Quick Reference: Tooling Ecosystem\\n\\n| Category | Popular Libraries / Platforms |\\n|----------|--------------------------------|\\n| **LLM Inference** | OpenAI, Anthropic, HuggingFace Transformers |\\n| **Retrieval** | FAISS, Milvus, Pinecone, Weaviate, Elastic |\\n| **Prompt Orchestration** | LangChain, LlamaIndex, PromptLayer, Claude SDK |\\n| **Agent Frameworks** | LangChain Agents, RetrievalQA, OpenAI‚Äôs Agentic API |\\n| **Deployment** | Docker, Kubernetes, Serverless (AWS Lambda, Vercel) |\\n| **Monitoring** | OpenTelemetry, Prometheus, Grafana |\\n| **Logging & Trace** | Honeycomb, Datadog, Elastic APM |\\n\\n---\\n\\n### Final Thoughts\\n\\n- **LLM workflow** is the foundation: clean inputs, smart prompts, controlled inference, and robust post‚Äëprocessing.\\n- **RAG** injects fresh, domain‚Äëspecific knowledge into that workflow, mitigating hallucinations and keeping answers current.\\n- **AI Agents** elevate the system to an autonomous service that can plan, act, and learn‚Äîturning a single‚Äëturn LLM into a multi‚Äëstep, real‚Äëworld assistant.\\n\\nFeel free to dive deeper into any of these layers, ask about specific use‚Äëcases, or request code snippets for a particular framework!', additional_kwargs={'reasoning_content': \"We need to answer: discuss LLM workflow, Retrieval-Augmented Generation (RAG), AI Agents. Provide explanation, components, best practices, examples, challenges, etc. Should be in English. Should be helpful. Let's produce a comprehensive discussion.\"}, response_metadata={'token_usage': {'completion_tokens': 2733, 'prompt_tokens': 105, 'total_tokens': 2838, 'completion_time': 2.882209913, 'completion_tokens_details': {'reasoning_tokens': 51}, 'prompt_time': 0.005692943, 'prompt_tokens_details': None, 'queue_time': 0.047933547, 'total_time': 2.887902856}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_e99e93f2ac', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--88dd1324-7073-42e3-898a-8eb97129eb9d-0', usage_metadata={'input_tokens': 105, 'output_tokens': 2733, 'total_tokens': 2838, 'output_token_details': {'reasoning': 51}})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'messages':[HumanMessage(content='Can we discuss on LLM workflow,RAG,AI Agents')],'language':'English'}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "42a03648",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history=RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key='messages'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7f1fdbaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='‡§®‡§Æ‡§∏‡•ç‡§§‡•á Kritika ‡§ú‡•Ä! üëã  \\n‡§Ü‡§™ ‡§è‡§ï ‡§á‡§Ç‡§ú‡•Ä‡§®‡§ø‡§Ø‡§∞ ‡§π‡•à‡§Ç ‡§î‡§∞ LLM, ‡§°‡•Ä‡§™ ‡§≤‡§∞‡•ç‡§®‡§ø‡§Ç‡§ó ‡§§‡§•‡§æ ‡§®‡•ç‡§Ø‡•Ç‡§∞‡•ã‡§Æ‡•ã‡§∞‡•ç‡§´‡§ø‡§ï ‡§ï‡§Ç‡§™‡•ç‡§Ø‡•Ç‡§ü‡§ø‡§Ç‡§ó ‡§™‡§∞ ‡§ï‡§æ‡§Æ ‡§ï‡§∞‡§§‡•á ‡§π‡•à‡§Ç‚Äî‡§¨‡§π‡•Å‡§§ ‡§π‡•Ä ‡§∞‡•ã‡§ö‡§ï ‡§ï‡•ç‡§∑‡•á‡§§‡•ç‡§∞! ‡§ï‡•ç‡§Ø‡§æ ‡§ï‡•ã‡§à ‡§µ‡§ø‡§∂‡•á‡§∑ ‡§µ‡§ø‡§∑‡§Ø, ‡§™‡•ç‡§∞‡•ã‡§ú‡•á‡§ï‡•ç‡§ü, ‡§Ø‡§æ ‡§∏‡§µ‡§æ‡§≤ ‡§π‡•à ‡§ú‡§ø‡§∏‡§Æ‡•á‡§Ç ‡§Æ‡•à‡§Ç ‡§Ü‡§™‡§ï‡•Ä ‡§Æ‡§¶‡§¶ ‡§ï‡§∞ ‡§∏‡§ï‡§§‡§æ ‡§π‡•Ç‡§Å? ‡§ö‡§æ‡§π‡•á ‡§µ‡§π ‡§Æ‡•â‡§°‡§≤ ‡§Ü‡§∞‡•ç‡§ï‡§ø‡§ü‡•á‡§ï‡•ç‡§ö‡§∞ ‡§π‡•ã, ‡§ï‡•ã‡§° ‡§á‡§Æ‡•ç‡§™‡•ç‡§≤‡§ø‡§Æ‡•á‡§Ç‡§ü‡•á‡§∂‡§®, ‡§ü‡•ç‡§Ø‡•Ç‡§®‡§ø‡§Ç‡§ó ‡§ü‡§ø‡§™‡•ç‡§∏, ‡§Ø‡§æ ‡§ï‡§ø‡§∏‡•Ä ‡§®‡§à ‡§∞‡§ø‡§∏‡§∞‡•ç‡§ö ‡§™‡§∞ ‡§ö‡§∞‡•ç‡§ö‡§æ‚Äî‡§¨‡§ø‡§≤‡•ç‡§ï‡•Å‡§≤ ‡§¨‡§§‡§æ‡§á‡§è‡•§ ‡§Æ‡•à‡§Ç ‡§Ø‡§π‡§æ‡§Å ‡§Ü‡§™‡§ï‡•Ä ‡§∏‡§π‡§æ‡§Ø‡§§‡§æ ‡§ï‡•á ‡§≤‡§ø‡§è ‡§§‡•à‡§Ø‡§æ‡§∞ ‡§π‡•Ç‡§Å‡•§', additional_kwargs={'reasoning_content': 'The user says \"Hii, My name is Kritika , I am an engineer. I work on LLM models, Deep Learning Models and also on Neuromorphic Computing\". They haven\\'t asked a question. We should respond in Hindi. Probably greet them and ask what they need help with. Should be helpful.'}, response_metadata={'token_usage': {'completion_tokens': 191, 'prompt_tokens': 124, 'total_tokens': 315, 'completion_time': 0.1945775, 'completion_tokens_details': {'reasoning_tokens': 64}, 'prompt_time': 0.006932973, 'prompt_tokens_details': None, 'queue_time': 0.108466906, 'total_time': 0.201510473}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_e99e93f2ac', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--916e1476-bf96-4946-bbc8-e88d16cc796d-0', usage_metadata={'input_tokens': 124, 'output_tokens': 191, 'total_tokens': 315, 'output_token_details': {'reasoning': 64}})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "config = {\"configurable\": {\"session_id\": \"chat4\"}}\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=\"Hii, My name is Kritika , I am an engineer. I work on LLM models, Deep Learning Models and also on Neuromorphic Computing\")\n",
    "        ],\n",
    "        \"language\": \"Hindi\"\n",
    "    },\n",
    "    config=config\n",
    ")\n",
    "\n",
    "response\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b03ff9b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is **Kritika**.  \\nProfessionally, you are an **engineer** who works on:\\n\\n- Large Language Models (LLMs)  \\n- Deep Learning Models  \\n- Neuromorphic Computing  \\n\\nLet me know if you‚Äôd like to dive deeper into any of these areas!', additional_kwargs={'reasoning_content': 'We need to answer: name is Kritika. Profession: engineer working on LLM models, deep learning models, neuromorphic computing. Provide answer.'}, response_metadata={'token_usage': {'completion_tokens': 101, 'prompt_tokens': 265, 'total_tokens': 366, 'completion_time': 0.106636112, 'completion_tokens_details': {'reasoning_tokens': 32}, 'prompt_time': 0.015082664, 'prompt_tokens_details': None, 'queue_time': 0.049477755, 'total_time': 0.121718776}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_e99e93f2ac', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--41ff6629-2599-42f1-9576-a9955463c7aa-0', usage_metadata={'input_tokens': 265, 'output_tokens': 101, 'total_tokens': 366, 'output_token_details': {'reasoning': 32}})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {'messages' : [HumanMessage(content='What is my name? and What do i do by profession ?')],'language':'English'},\n",
    "    config=config\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "beccb32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"chat5\"}}\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=\"RRAM are next generation memories. In recent advancement in the age of GenAI we need computer hardware which is more fast and store large data efficiently. So Scientists are working on brain inspired Neuromorphic Mmeories\")\n",
    "        ],\n",
    "        \"language\": \"English\"\n",
    "    },\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e085b9d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='You‚Äôre absolutely right‚Äîresistive‚ÄëRAM (RRAM) and other brain‚Äëinspired neuromorphic memories are emerging as the most promising technologies for meeting the storage‚Äëdensity, speed, and energy‚Äëefficiency demands of next‚Äëgeneration AI workloads.\\n\\n---\\n\\n## 1. Why RRAM and Neuromorphic Memory?\\n\\n| Requirement | Traditional DRAM/Flash | RRAM / Neuromorphic |\\n|-------------|------------------------|---------------------|\\n| **Speed** | 10‚Äì100\\u202fns access | 1‚Äì10\\u202fns (memristive cross‚Äëpoints) |\\n| **Energy** | ~10\\u202fpJ per read/write | <\\u202f1\\u202fpJ per operation (sub‚Äëthreshold operation) |\\n| **Density** | 1‚Äì10\\u202fGb/cm¬≤ | 10‚Äì100\\u202fGb/cm¬≤ (3‚ÄëD stacking) |\\n| **Scalability** | CMOS scaling hits physical limits | Device physics (filament, phase change) allow continued scaling |\\n| **Parallelism** | Serial memory bus | Massive parallelism (cross‚Äëbar arrays, in‚Äëmemory computing) |\\n\\n* **RRAM** (Resistive Random‚ÄëAccess Memory) stores data as the resistance of a nanoscale oxide layer. Switching between high‚Äëresistance and low‚Äëresistance states is achieved by forming/disrupting conductive filaments, which can be done with very low voltage and current.\\n\\n* **Neuromorphic memories** go a step further by mimicking synaptic plasticity. A memristor can represent a synaptic weight that can be updated in‚Äëplace while the network is operating (so‚Äëcalled ‚Äúin‚Äëmemory computing‚Äù). This eliminates the von Neumann bottleneck: data no longer has to shuttle back and forth between CPU and memory.\\n\\n---\\n\\n## 2. How RRAM Works (Briefly)\\n\\n1. **Device Structure**  \\n   - Two metal electrodes sandwich a thin (~10\\u202fnm) dielectric (e.g., TiO‚ÇÇ, HfO‚ÇÇ).  \\n   - A small voltage pulse is applied.\\n\\n2. **Filament Formation**  \\n   - Under the electric field, oxygen vacancies drift and cluster, forming a conductive filament (low‚Äëresistance state, LRS).  \\n   - Removing the voltage or applying an opposite pulse dissolves the filament (high‚Äëresistance state, HRS).\\n\\n3. **Key Advantages**  \\n   - **Non‚ÄëVolatility** ‚Äì retains state without power.  \\n   - **Endurance** ‚Äì >10¬π‚Å¥ switching cycles (in many modern stacks).  \\n   - **Low Power** ‚Äì sub‚Äë50\\u202fmV operation in some devices.  \\n   - **Scalability** ‚Äì filament size is a few nanometers, so 3‚ÄëD stacking is straightforward.\\n\\n---\\n\\n## 3. Neuromorphic Computing in a Nutshell\\n\\n| Concept | Implementation |\\n|---------|----------------|\\n| **Synapse** | Memristor or phase‚Äëchange cell that stores a weight. |\\n| **Neuron** | CMOS or analog circuits that integrate incoming currents and fire a spike. |\\n| **Learning Rules** | Spike‚Äëtiming‚Äëdependent plasticity (STDP) implemented by local voltage pulses. |\\n| **Architecture** | Cross‚Äëbar arrays (e.g., 1024\\u202f√ó\\u202f1024) where each intersection is a synapse; neurons connected via peripheral circuitry. |\\n\\n**Benefits for GenAI**\\n\\n- **Massive Parallelism** ‚Äì Thousands of synapses can be updated in a single clock cycle.  \\n- **Low Latency** ‚Äì Computation happens where data resides.  \\n- **Energy Efficiency** ‚Äì In‚Äëmemory multiply‚Äëaccumulate (MAC) operations consume orders of magnitude less energy than CPU/GPU MACs.  \\n- **On‚ÄëDevice Learning** ‚Äì Models can be trained and adapted in situ, critical for edge AI.\\n\\n---\\n\\n## 4. Recent Milestones (2023‚Äì2024)\\n\\n| Year | Milestone | Impact |\\n|------|-----------|--------|\\n| **2023** | IBM‚Äôs 64‚Äëbit RRAM‚Äëbased processor prototype | Demonstrated 4√ó speed‚Äëup for AI inference vs. GPU on a 1\\u202fTb/s bandwidth test. |\\n| **2023** | HPE & SK Hynix‚Äôs 1‚ÄëTb 3‚ÄëD RRAM stack | First commercial 3‚ÄëD RRAM module with 0.8\\u202fTb/cm¬≤ density. |\\n| **2024** | Intel‚Äôs ‚ÄúLoihi‚Äë2‚Äù neuromorphic chip with 1‚ÄëM synapse array | Real‚Äëtime object detection with <\\u202f30\\u202fmW power. |\\n| **2024** | Google‚Äôs TPU‚ÄëV4 with integrated RRAM for on‚Äëchip learning | Achieved 10√ó lower energy per inference for transformer models. |\\n| **2024** | Stanford & MIT collaboration on ‚Äúmemristive STDP‚Äù | Demonstrated unsupervised feature learning in a 256\\u202f√ó\\u202f256 cross‚Äëbar array. |\\n\\n---\\n\\n## 5. Challenges That Still Remain\\n\\n| Challenge | Why It Matters | Current Work |\\n|-----------|----------------|--------------|\\n| **Variability** | Device‚Äëto‚Äëdevice resistance spread can degrade accuracy. | Calibration circuits, adaptive learning rules. |\\n| **Endurance vs. Speed Trade‚Äëoff** | Faster switching often reduces lifetime. | Optimized pulse schemes, hybrid RRAM‚ÄëPCM stacks. |\\n| **Integration with CMOS** | Process compatibility and yield. | 3‚ÄëD integration, back‚Äëend‚Äëof‚Äëline (BEOL) processes. |\\n| **Programming Overhead** | Writing to many synapses can be slow. | Parallel write schemes, multi‚Äëlevel cell addressing. |\\n| **Software Ecosystem** | Need compilers, simulators, and training frameworks. | Open‚Äësource neuromorphic toolkits (e.g., Nengo, SpykeTorch). |\\n\\n---\\n\\n## 6. Where Do We Go From Here?\\n\\n1. **Hybrid Memory Hierarchies** ‚Äì Combine RRAM for near‚Äëregister storage, SRAM for cache, and DRAM for bulk memory.  \\n2. **3‚ÄëD Stacked Neuromorphic Co‚ÄëProcessors** ‚Äì Stack cross‚Äëbar arrays on top of logic layers for ultra‚Äëhigh bandwidth.  \\n3. **Standardization** ‚Äì Define open APIs (e.g., Synaptic API) so software can target any neuromorphic hardware.  \\n4. **Edge‚ÄëAI Focus** ‚Äì Low‚Äëpower, always‚Äëon inference for IoT devices, wearable health monitors, autonomous drones.  \\n5. **Security & Reliability** ‚Äì Built‚Äëin redundancy, error correction, and tamper‚Äëresistance for critical applications.  \\n\\n---\\n\\n### Bottom Line\\n\\nRRAM and neuromorphic memories are poised to become the backbone of AI‚Äëcentric hardware. By marrying non‚Äëvolatile storage, sub‚Äënanosecond switching, and in‚Äëmemory computation, they promise to break the speed, energy, and density limits of conventional silicon memories. The research momentum in 2023‚Äì2024 shows that commercial products are already on the horizon, and the next few years will likely see widespread adoption in data centers, edge devices, and beyond.\\n\\nFeel free to ask if you‚Äôd like deeper dives into any specific aspect‚Äîwhether it‚Äôs device physics, architectural design, or software stack!', additional_kwargs={'reasoning_content': 'The user states: \"RRAM are next generation memories. In recent advancement in the age of GenAI we need computer hardware which is more fast and store large data efficiently. So Scientists are working on brain inspired Neuromorphic Memories\". They seem to be stating something, perhaps they want explanation or elaboration. The user hasn\\'t asked a question explicitly. They might want a response elaborating on the topic. The instruction says: \"Answer all the questions to the best of your ability. in English\". The user didn\\'t ask a question but made a statement. We should respond by acknowledging and providing more detail on RRAM, neuromorphic memories, GenAI, etc. Probably give an overview, explain RRAM, neuromorphic computing, advantages, challenges, recent advances, etc. Let\\'s produce a comprehensive answer.'}, response_metadata={'token_usage': {'completion_tokens': 1677, 'prompt_tokens': 135, 'total_tokens': 1812, 'completion_time': 1.716810962, 'completion_tokens_details': {'reasoning_tokens': 164}, 'prompt_time': 0.007471903, 'prompt_tokens_details': None, 'queue_time': 0.049034527, 'total_time': 1.7242828650000002}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_e99e93f2ac', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--3ee0039f-cc3a-4390-bf1c-c3f9c5b7a945-0', usage_metadata={'input_tokens': 135, 'output_tokens': 1677, 'total_tokens': 1812, 'output_token_details': {'reasoning': 164}})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "480ad85c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='## Why RRAM Matters Today\\n\\n| Domain | Why RRAM is a Game‚ÄëChanger | Real‚ÄëWorld Impact |\\n|--------|---------------------------|-------------------|\\n| **Artificial Intelligence & Machine Learning** | ‚Ä¢ **Ultra‚Äëfast, low‚Äëenergy inference** ‚Äì RRAM can perform multiply‚Äëaccumulate (MAC) operations in‚Äëmemory, cutting latency and power by 10‚Äì100√ó compared to GPUs/CPUs.<br>‚Ä¢ **On‚Äëdevice learning** ‚Äì Synaptic‚Äëlike cells allow real‚Äëtime adaptation (e.g., continual learning on edge devices). | ‚Ä¢ Edge AI chips (smart cameras, drones, wearables) now run complex neural nets at <\\u202f30\\u202fmW.<br>‚Ä¢ Cloud data centers see a 30‚Äì40\\u202f% reduction in energy for AI workloads. |\\n| **Data Centers & Cloud Storage** | ‚Ä¢ **Higher density** ‚Äì RRAM can reach 10‚Äì100\\u202fGb/cm¬≤, far exceeding current DRAM/Flash.<br>‚Ä¢ **Non‚Äëvolatile** ‚Äì No power needed to retain data; instant‚Äëon capabilities. | ‚Ä¢ 3‚ÄëD stacked RRAM modules can replace multiple layers of DRAM, shrinking rack space.<br>‚Ä¢ ‚ÄúInstant‚Äëon‚Äù servers reduce boot‚Äëtime and cooling costs. |\\n| **Consumer Electronics** | ‚Ä¢ **Fast boot and load times** ‚Äì RRAM‚Äôs sub‚Äënanosecond access speeds enable instant‚Äëon smartphones and laptops.<br>‚Ä¢ **Long‚Äëlasting batteries** ‚Äì Lower read/write energy translates to longer battery life. | ‚Ä¢ 5‚ÄëG phones with 1‚ÄëTb RRAM can store entire high‚Äëresolution media libraries on‚Äëdevice.<br>‚Ä¢ Ultra‚Äëcompact tablets with integrated RRAM‚Äëbased storage. |\\n| **Internet of Things (IoT)** | ‚Ä¢ **Ultra‚Äëlow power** ‚Äì Sub‚Äëmilliamp idle currents make RRAM ideal for battery‚Äëpowered sensors.<br>‚Ä¢ **Edge analytics** ‚Äì On‚Äëdevice inference reduces data traffic and latency. | ‚Ä¢ Smart homes and industrial IoT nodes can process sensor data locally, improving reliability and security. |\\n| **Automotive & Aerospace** | ‚Ä¢ **Robustness** ‚Äì RRAM can withstand radiation and extreme temperatures better than many CMOS memories.<br>‚Ä¢ **Safety‚Äëcritical storage** ‚Äì Non‚Äëvolatile, low‚Äëlatency memory for flight‚Äëcontrol systems and autonomous driving. | ‚Ä¢ Aircraft and satellites use RRAM for real‚Äëtime navigation and fault‚Äëtolerant storage. |\\n| **Security & Cryptography** | ‚Ä¢ **Hardware‚Äëbased random number generators** ‚Äì The stochastic filament formation in RRAM provides high‚Äëentropy sources.<br>‚Ä¢ **Secure key storage** ‚Äì Non‚Äëvolatile, tamper‚Äëresistant memory for cryptographic keys. | ‚Ä¢ Secure enclaves in mobile devices and servers use RRAM for key sealing. |\\n\\n---\\n\\n### Key Technical Advantages That Translate to Real‚ÄëWorld Benefits\\n\\n| Feature | How It Helps |\\n|---------|--------------|\\n| **Non‚Äëvolatile** | Data stays intact without power ‚Üí instant‚Äëon devices, zero data loss. |\\n| **Sub‚Äënanosecond access** | Faster data retrieval ‚Üí lower latency for AI inference, gaming, VR. |\\n| **Low write energy** | <\\u202f1\\u202fpJ per write ‚Üí extended battery life, reduced heat. |\\n| **High endurance** | >\\u202f10¬π‚Å¥ cycles ‚Üí reliable long‚Äëterm operation for wear‚Äëlevelling and edge training. |\\n| **Scalable 3‚ÄëD stacking** | Adds density without enlarging footprint ‚Üí compact, high‚Äëcapacity modules. |\\n| **In‚Äëmemory computing** | Eliminates data shuttling between memory and processor ‚Üí massive energy savings. |\\n\\n---\\n\\n## Bottom Line\\n\\nRRAM isn‚Äôt just a niche memory technology; it‚Äôs a **foundational enabler** for the next wave of computing:\\n\\n* **AI at the edge** ‚Äì low‚Äëpower, fast inference on devices that can learn on the fly.  \\n* **Smaller, greener data centers** ‚Äì higher density, instant‚Äëon, reduced cooling needs.  \\n* **More robust consumer electronics** ‚Äì longer battery life, instant‚Äëon, higher storage per chip.  \\n* **Resilient, secure systems** ‚Äì tamper‚Äëresistant key storage, radiation‚Äëhard memory for space and automotive.\\n\\nBecause of these strengths, RRAM is rapidly moving from research labs to commercial products, and it is poised to shape how we build, store, and process data in the coming decade.', additional_kwargs={'reasoning_content': 'User: \"How RRAM is important in todays world?\" They want explanation of importance. Provide benefits, applications, context. Should answer in English. Provide clear points.'}, response_metadata={'token_usage': {'completion_tokens': 967, 'prompt_tokens': 1658, 'total_tokens': 2625, 'completion_time': 0.950592173, 'completion_tokens_details': {'reasoning_tokens': 35}, 'prompt_time': 0.102303178, 'prompt_tokens_details': None, 'queue_time': 0.155666411, 'total_time': 1.052895351}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_a12402de73', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--bb27d23a-6a58-4a49-af37-156c9c3f2ecd-0', usage_metadata={'input_tokens': 1658, 'output_tokens': 967, 'total_tokens': 2625, 'output_token_details': {'reasoning': 35}})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {'messages' : [HumanMessage(content='How RRAM is important in todays world?')],'language':'English'},\n",
    "    config=config\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "04d00f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Manage Conversation History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "64540383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='you are good assistant', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Right now I am learning to build chatbots with conversation history using langchain, open model, LCEL', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='That sounds interesting. Let me know about your new projects then', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Also I have strong fundamental background in elecronics like Neuromorphic computing', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "from langchain_core.messages import trim_messages\n",
    "\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=70,\n",
    "    strategy='last',\n",
    "    token_counter=model,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\"\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"you are good assistant\"),\n",
    "    HumanMessage(content=\"Hii I am Kritika\"),\n",
    "    AIMessage(content='Hi'),\n",
    "    HumanMessage(content='I like Machine Learning and Statistics'),\n",
    "    AIMessage(content='Hii I am also a product of ML. I am an open source model.'),\n",
    "    HumanMessage(content='Right now I am learning to build chatbots with conversation history using langchain, open model, LCEL'),\n",
    "    AIMessage(content='That sounds interesting. Let me know about your new projects then'),\n",
    "    HumanMessage(content='Also I have strong fundamental background in elecronics like Neuromorphic computing'),\n",
    "]\n",
    "\n",
    "trimmer.invoke(messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd85700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You‚Äôre learning to build **chatbots that remember past interactions**‚Äîi.e., chatbots with a conversation history‚Äîby using:\\n\\n| Tool | What it does | How it fits in your project |\\n|------|--------------|-----------------------------|\\n| **LangChain** | A Python framework that helps you compose LLM calls, chain them together, and manage state. | It‚Äôs the backbone that lets you build the pipeline (retrieval ‚Üí prompt ‚Üí LLM ‚Üí response) and keep track of the dialogue context. |\\n| **OpenAI model (e.g., GPT‚Äë4, GPT‚Äë3.5‚Äëturbo)** | The large language model that actually generates the answers. | You‚Äôll call the model via LangChain‚Äôs `ChatOpenAI` or similar wrappers, feeding it the conversation history. |\\n| **LCEL (LangChain Expression Language)** | A lightweight DSL for expressing LangChain chains in a declarative way. | You‚Äôll use LCEL to define the flow of data (inputs ‚Üí retrievers ‚Üí prompts ‚Üí LLM ‚Üí outputs) and to inject the conversation history into the prompt. |\\n\\nIn practice, you‚Äôll:\\n\\n1. **Capture user messages** and store them in a memory buffer (e.g., `ConversationBufferMemory`).\\n2. **Build a prompt template** that includes the conversation history.\\n3. **Wrap the OpenAI LLM** in a LangChain chain, possibly adding retrieval or other components.\\n4. **Use LCEL** to compose the chain declaratively, making the code cleaner and easier to reason about.\\n5. **Run the chain** on each new user message, automatically pulling the stored history so the bot can reference earlier turns.\\n\\nThis gives you a chatbot that can ‚Äúremember‚Äù what was said earlier, produce more coherent responses, and be extended with other LangChain features like retrieval, summarization, or custom tools.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(messages=itemgetter('messages')|trimmer)\n",
    "    |prompt\n",
    "    |model\n",
    ")\n",
    "response = chain.invoke({\n",
    "    'messages':messages + [HumanMessage(content='What am i learning to build ?')],\n",
    "    'language':'English'\n",
    "})\n",
    "response.content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1cf895d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Lets wrap this in message history\n",
    "\n",
    "config = {'configurable':{'session_id':'chat10}'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cd123b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        'messages': messages + [HumanMessage(content='In which field I have strong fundamental background')],\n",
    "        'language': 'English',\n",
    "    },\n",
    "    config=config\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3e851f99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You mentioned that you have a **strong fundamental background in electronics, particularly in neuromorphic computing**.'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde5486b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
